<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Multi-Class Prediction of Obesity Risk</title>
    <meta name="description" content="Sixth day of my Kaggle challenge with a multi-class prediction model for obesity risk using various ML techniques and tuning XGBoost.">
    <meta name="author" content="Suraj Wate">
    <meta name="keywords" content="Data Science, Machine Learning, Python">

    <!-- Info for Eleventy Leaderboard -->
    <meta name="generator" content="Eleventy">

    <!-- Open Graph Tags -->
    
    <meta property="og:title" content="Multi-Class Prediction of Obesity Risk">
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://surajwate.com/blog/multi-class-prediction-of-obesity-risk/">
    <meta property="og:description" content="Sixth day of my Kaggle challenge with a multi-class prediction model for obesity risk using various ML techniques and tuning XGBoost.">
    <meta property="og:image" content="/assets/images/Multi-Class-Obesity-Prediction-with-ML.png">
    <meta property="og:site_name" content="Suraj Wate's Blog">

    
    <!-- Twitter Card Tags -->
    
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:title" content="Multi-Class Prediction of Obesity Risk">
        <meta name="twitter:description" content="Sixth day of my Kaggle challenge with a multi-class prediction model for obesity risk using various ML techniques and tuning XGBoost.">
        <meta name="twitter:image" content="/assets/images/Multi-Class-Obesity-Prediction-with-ML.png">
    

    <link rel="stylesheet" href="/assets/css/base.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css"> <!-- Prism CSS -->
    <!-- Font Awesome for Icons -->
    <script defer="" src="https://kit.fontawesome.com/c39a85ddd3.js" crossorigin="anonymous"></script>
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/images/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/images/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/images/icons/favicon-16x16.png">
    <link rel="manifest" href="/assets/images/icons/site.webmanifest">

    <!-- IndieAuth -->
    <link rel="me" href="https://fosstodon.org/@suraj">
    <link rel="me" href="https://github.com/surajwate">
    <link rel="me" href="https://twitter.com/surajwate">
    <link rel="me" href="https://linkedin.com/in/surajwate">
    <link rel="webmention" href="https://webmention.io/surajwate.com/webmention">

    <link rel="authorization_endpoint" href="https://indieauth.com/auth">
    <link rel="token_endpoint" href="https://tokens.indieauth.com/token">
    <link rel="microsub" href="https://aperture.p3k.io/microsub/978">

     
        
    <link rel="stylesheet" href="/assets/css/post.css">

    <!-- Custom Prism CSS -->
    <link rel="stylesheet" href="/assets/css/prism-custom.css">
    <!--- Custom Table Styles -->
    <link rel="stylesheet" href="/assets/css/table-styles.css">

    <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [
                    ["$", "$"],
                    ["\\(", "\\)"],
                ],
                displayMath: [
                    ["$$", "$$"],
                    ["\\[", "\\]"],
                ],
            },
        };
    </script>

    <script type="application/ld+json">
        {
            "@context": "https://schema.org",
            "@type": "Article",
            "headline": "Multi-Class Prediction of Obesity Risk",
            "description": "Sixth day of my Kaggle challenge with a multi-class prediction model for obesity risk using various ML techniques and tuning XGBoost.",
            "datePublished": "Mon Sep 16 2024 00:00:00 GMT+0000 (Coordinated Universal Time)",
            "author": {
                "@type": "Person",
                "name": "Suraj Wate"
            },
            "publisher": {
                "@type": "Person",
                "name": "Suraj Wate's Blog"
            },
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "https://surajwate.com/blog/multi-class-prediction-of-obesity-risk/"
            },
            "image": "/assets/images/Multi-Class-Obesity-Prediction-with-ML.png",
            "articleSection": "Kaggle Challenges, Machine Learning Projects",
            "keywords": "Multiclass-Classification, XGBoost, Model-Stacking, 30-Kaggle-Challenges, Hyperparameter-Tuning",
            "inLanguage": "en"
        }
    </script>



    <!-- Google tag (gtag.js) -->
    <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-3VGQDQ5YQV"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-3VGQDQ5YQV');
    </script>
</head>
<body>
    <header>
        <nav class="navbar">
            <a href="/" class="logo">Suraj Wate</a>
            <button class="nav-toggle" aria-label="Toggle navigation">
                <span class="hamburger"></span>
            </button>
            <ul class="nav-links">
                <li><a href="/blog"><i class="fa-regular fa-newspaper"></i> Blog</a></li>
                <li><a href="/projects"><i class="fa-solid fa-laptop-code"></i> Projects</a></li>
                <li><a href="/about"><i class="fas fa-user"></i> About Me</a></li>
                <li><a href="/contact"><i class="fas fa-envelope"></i> Contact</a></li>
                    
            </ul>
        </nav>
    </header>
    
    <main>
        
    <div class="post-container container">
        <h1>Multi-Class Prediction of Obesity Risk</h1>
        <p class="post-date">Sep 16, 2024</p>
        <h2>Introduction</h2>
<p>Today is the sixth day of the <a href="https://surajwate.com/projects/30-days-of-kaggle-challenges/">30 Kaggle Challenges in 30 Days</a> challenge. I am wondering if I should keep these blogs static or dynamic. I am building very basic ML models right now. Should I come back to these blogs in the future and add new ML models by updating them, or should I create new ones later on and link them to these blogs? I am unsure of it right now. Well, I will add this to my to-do list. Returning to the challenge, it is a multi-class prediction model. Keep in mind that it’s multi-class and not multi-label. There is a difference between the two. We will explore this difference as we solve the problem.</p>
<h2>Data Overview</h2>
<p>Kaggle Dataset: <a href="https://www.kaggle.com/competitions/playground-series-s4e2/data">Season 4, Episode 2</a><br>
Original dataset: <a href="https://www.kaggle.com/datasets/aravindpcoder/obesity-or-cvd-risk-classifyregressorcluster">Obesity or CVD risk</a></p>
<h3>Data Description</h3>
<p>The dataset for this competition is synthetically generated from Obesity or CVD risk dataset. The original data consists of the estimation of obesity levels in people from the countries of Mexico, Peru, and Colombia, with ages between 14 and 61 and diverse eating habits and physical conditions. The description of the columns is as follows:</p>
<ul>
<li><code>Gender</code>: Gender of the person</li>
<li><code>Age</code>: Age of the person</li>
<li><code>Height</code>: Height of the person</li>
<li><code>Weight</code>: Weight of the person</li>
<li><code>family_history_with_overweight</code>: If the person has a family history of being overweight</li>
<li><code>FAVC</code>: Frequent consumption of high-caloric food</li>
<li><code>FCVC</code>: Frequency of consumption of vegetables</li>
<li><code>NCP</code>: Number of main meals</li>
<li><code>CAEC</code>: Consumption of food between meals</li>
<li><code>SMOKE</code>: If the person smokes</li>
<li><code>CH2O</code>: Consumption of water daily</li>
<li><code>SCC</code>: Calories consumption monitoring</li>
<li><code>FAF</code>: Physical activity frequency</li>
<li><code>TUE</code>: Time using technology devices</li>
<li><code>CALC</code>: Consumption of alcohol</li>
<li><code>MTRANS</code>: Transportation used</li>
<li><code>NObeyesdad</code>: Obesity level</li>
</ul>
<h3>Kaggle Datasets</h3>
<p>Kaggle has provided three data files for this competition:</p>
<ul>
<li><code>train.csv</code>: This training dataset contains the features and the target variable.</li>
<li><code>test.csv</code>: This test dataset contains the features.</li>
<li><code>sample_submission.csv</code>: This sample submission file shows the correct submission format.</li>
</ul>
<h3>Data Files</h3>
<table>
<thead>
<tr>
<th>File</th>
<th>Rows</th>
<th>Columns</th>
</tr>
</thead>
<tbody>
<tr>
<td>train.csv</td>
<td>20758</td>
<td>18</td>
</tr>
<tr>
<td>test.csv</td>
<td>13840</td>
<td>17</td>
</tr>
</tbody>
</table>
<p>The training dataset has 18 columns: 16 are features, 1 is ID, and 1 is the target variable, <code>NObeyesdad</code>.</p>
<h3>Variable Details</h3>
<p>From a total of 17 variables, 8 are categorical features, 8 numerical features and 1 target variable.</p>
<h4>Missing Values</h4>
<p>Check the training set for missing values. The dataset does not have any missing values.</p>
<h4>Target Variable</h4>
<p>Data have 7 levels of obesity.</p>
<pre class="language-txt"><code class="language-txt">Obesity_Type_III       4046
Obesity_Type_II        3248
Normal_Weight          3082
Obesity_Type_I         2910
Insufficient_Weight    2523
Overweight_Level_II    2522
Overweight_Level_I     2427</code></pre>
<p><img src="/assets/images/Kaggle-S4E2-Level-of-Obesity.png" alt="Kaggle S4E2: Distribution of Level of Obesity" loading="lazy"></p>
<p>Although the target variable doesn’t have a huge imbalance, we will still monitor its effect on the model’s performance.</p>
<h4>Gender</h4>
<p><img src="/assets/images/Kaggle-S4E2-Distribution-of-Gender.png" alt="Kaggle S4E2: Distribution of Gender by Level of Obesity" loading="lazy"></p>
<p>Most females have obesity type III, while obesity type II is prevalent in males.</p>
<h4>Other Variables Features</h4>
<p>I can’t upload all the plots, but if you are interested you can check the notebook in GitHub. I will write the detail analysis of all the features.</p>
<p>91.44% of people said that they frequently consume high-calorie food. 84.44% said they do consume food between the means sometimes. 98.82% of people smoke.</p>
<p>The majority of the respondents are in their 20s. The respondents’ heights are normally distributed, with the mean and median both around 1.7 meters. The weight distribution is spread out but doesn’t have any outliers. The number of meals has 29.15% outliers. It ranges from 1 to 3 meals a day, with most people consuming three meals per day.</p>
<h2>Preprocessing</h2>
<p>There are 8 categorical features in total, which will be encoded using one hot encoding. The scaling will be done to numerical features. Only one feature has outliers, but the values are understandable, as some people might have only one meal due to diet or any other reason, and it’s perfectly possible for some to have four meals.</p>
<h2>Model Selection</h2>
<ul>
<li>Discuss the model considered and the reasons for choosing specific algorithms.</li>
<li>Include a brief description of hyperparameters tuning or techniques like cross-validation.</li>
</ul>
<h2>Evaluation</h2>
<p>Here’s a table summarizing the accuracy scores from each model across the 5-folds:</p>
<table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>Fold 0</strong></th>
<th><strong>Fold 1</strong></th>
<th><strong>Fold 2</strong></th>
<th><strong>Fold 3</strong></th>
<th><strong>Fold 4</strong></th>
<th><strong>Average Accuracy</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Logistic Regression</strong></td>
<td>0.8687</td>
<td>0.8651</td>
<td>0.8683</td>
<td>0.8533</td>
<td>0.8598</td>
<td>0.8630</td>
</tr>
<tr>
<td><strong>Random Forest</strong></td>
<td>0.8993</td>
<td>0.8878</td>
<td>0.8890</td>
<td>0.8841</td>
<td>0.8909</td>
<td>0.8902</td>
</tr>
<tr>
<td><strong>Decision Tree</strong></td>
<td>0.8408</td>
<td>0.8509</td>
<td>0.8413</td>
<td>0.8497</td>
<td>0.8511</td>
<td>0.8468</td>
</tr>
<tr>
<td><strong>SVM</strong></td>
<td>0.8866</td>
<td>0.8789</td>
<td>0.8779</td>
<td>0.8702</td>
<td>0.8755</td>
<td>0.8778</td>
</tr>
<tr>
<td><strong>Gradient Boosting</strong></td>
<td>0.9085</td>
<td>0.9082</td>
<td>0.9039</td>
<td>0.8993</td>
<td>0.9041</td>
<td>0.9048</td>
</tr>
<tr>
<td><strong>XGBoost</strong></td>
<td>0.9114</td>
<td>0.9102</td>
<td>0.9070</td>
<td>0.8991</td>
<td>0.9046</td>
<td>0.9065</td>
</tr>
<tr>
<td><strong>KNN</strong></td>
<td>0.7604</td>
<td>0.7577</td>
<td>0.7575</td>
<td>0.7572</td>
<td>0.7545</td>
<td>0.7575</td>
</tr>
<tr>
<td><strong>Extra Trees</strong></td>
<td>0.8666</td>
<td>0.8557</td>
<td>0.8615</td>
<td>0.8535</td>
<td>0.8624</td>
<td>0.8599</td>
</tr>
<tr>
<td><strong>AdaBoost</strong></td>
<td>0.4725</td>
<td>0.4321</td>
<td>0.6671</td>
<td>0.4054</td>
<td>0.6630</td>
<td>0.5280</td>
</tr>
<tr>
<td><strong>Bagging</strong></td>
<td>0.8875</td>
<td>0.8911</td>
<td>0.8849</td>
<td>0.8817</td>
<td>0.8851</td>
<td>0.8861</td>
</tr>
<tr>
<td><strong>LightGBM</strong></td>
<td>0.9128</td>
<td>0.9092</td>
<td>0.9063</td>
<td>0.9024</td>
<td>0.9015</td>
<td>0.9064</td>
</tr>
<tr>
<td><strong>CatBoost</strong></td>
<td>0.9128</td>
<td>0.9051</td>
<td>0.9063</td>
<td>0.8988</td>
<td>0.9085</td>
<td>0.9063</td>
</tr>
</tbody>
</table>
<h3>Key Takeaways:</h3>
<ul>
<li><strong>Best Performers</strong>: XGBoost, LightGBM, and CatBoost all achieved accuracy scores around <strong>90.6%</strong> to <strong>91.1%</strong>.</li>
<li><strong>Worst Performers</strong>: AdaBoost had a poor average accuracy of around <strong>52.8%</strong>.</li>
<li><strong>KNN</strong> also performed below expectations, with an average accuracy of <strong>75.75%</strong>.</li>
</ul>
<h3>Stacking</h3>
<p>Stacking, also called as stacked generalization, is using the output of the few base model and fit the results on another model called “meta model” to make predictions.<br>
I stacked the three top-performing models to check whether the score improved. I used logistic regression as the meta-model for stacking.</p>
<pre class="mermaid">graph TD;
    A[Input Data] --&gt; B[XGBoost]
    A[Input Data] --&gt; C[LightGBM]
    A[Input Data] --&gt; D[CatBoost]
    
    B --&gt; E[Meta Model: Logistic Regression]
    C --&gt; E[Meta Model: Logistic Regression]
    D --&gt; E[Meta Model: Logistic Regression]
    
    E --&gt; F[Final Prediction]
</pre>
<!-- Description for accessibility and SEO --> 
<p><strong>Diagram</strong>: This diagram shows the stacking of XGBoost, LightGBM, and CatBoost models with Logistic Regression as the meta-model, used for final prediction.</p>
<pre class="language-python"><code class="language-python">base_models <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">(</span>‘xgboost’<span class="token punctuation">,</span> XGBClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">(</span>‘lightgbm’<span class="token punctuation">,</span> LGBMClassifier<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">(</span><span class="token string">'catboost'</span><span class="token punctuation">,</span> CatBoostClassifier<span class="token punctuation">(</span>verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>

<span class="token comment"># Meta model</span>
meta_model <span class="token operator">=</span> LogisticRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Define the stacking classifier</span>
stacking_model <span class="token operator">=</span> StackingClassifier<span class="token punctuation">(</span>estimators<span class="token operator">=</span>base_models<span class="token punctuation">,</span> final_estimator<span class="token operator">=</span>meta_model<span class="token punctuation">)</span></code></pre>
<p>I got a slight improvement in the result by using stacking.</p>
<table>
<thead>
<tr>
<th>Meta Model</th>
<th>Fold 0</th>
<th>Fold 1</th>
<th>Fold 2</th>
<th>Fold 3</th>
<th>Fold 4</th>
<th>Average Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Logistic Regression</td>
<td>0.9133</td>
<td>0.9087</td>
<td>0.9078</td>
<td>0.8998</td>
<td>0.9060</td>
<td>0.90712</td>
</tr>
<tr>
<td>Random Forest</td>
<td>0.9104</td>
<td>0.9063</td>
<td>0.9087</td>
<td>0.9012</td>
<td>0.9034</td>
<td>0.90600</td>
</tr>
<tr>
<td>XGBClassifier</td>
<td>0.9106</td>
<td>0.9022</td>
<td>0.9058</td>
<td>0.8950</td>
<td>0.9020</td>
<td>0.90312</td>
</tr>
</tbody>
</table>
<p>This table compares performance across three different meta-models: Logistic Regression, Random Forest, and XGBClassifier.</p>
<h3>XGBoost</h3>
<p>The performance of stacking and XGBoost is very close. Since it’s efficient to run a random grid search on XGBoost, I selected it.<br>
Following are the results of a random grid search with XGBoost across different folds:</p>
<table>
<thead>
<tr>
<th>Fold 0</th>
<th>Fold 1</th>
<th>Fold 2</th>
<th>Fold 3</th>
<th>Fold 4</th>
<th>Average Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.9162</td>
<td>0.9106</td>
<td>0.9092</td>
<td>0.9020</td>
<td>0.9073</td>
<td>0.90906</td>
</tr>
</tbody>
</table>
<pre class="language-python"><code class="language-python"><span class="token punctuation">{</span><span class="token string">'model__subsample'</span><span class="token punctuation">:</span> <span class="token number">0.8</span><span class="token punctuation">,</span> <span class="token string">'model__n_estimators'</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span> <span class="token string">'model__max_depth'</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'model__learning_rate'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token string">'model__colsample_bytree'</span><span class="token punctuation">:</span> <span class="token number">1.0</span><span class="token punctuation">}</span></code></pre>
<p>I ran a grid search to improve my score. The following is the result. However, it took me around 71 minutes.</p>
<pre class="language-bash"><code class="language-bash">└─Δ python .<span class="token punctuation">\</span>src<span class="token punctuation">\</span>grid_search.py <span class="token parameter variable">--model</span> xgboost
Fitting <span class="token number">5</span> folds <span class="token keyword">for</span> each of <span class="token number">256</span> candidates, totalling <span class="token number">1280</span> fits
<span class="token assign-left variable">Fold</span><span class="token operator">=</span><span class="token number">0</span>, Accuracy <span class="token operator">=</span> <span class="token number">0.9135</span>, Time <span class="token assign-left variable">Taken</span><span class="token operator">=</span><span class="token number">881</span>.79sec
Fitting <span class="token number">5</span> folds <span class="token keyword">for</span> each of <span class="token number">256</span> candidates, totalling <span class="token number">1280</span> fits
<span class="token assign-left variable">Fold</span><span class="token operator">=</span><span class="token number">1</span>, Accuracy <span class="token operator">=</span> <span class="token number">0.9126</span>, Time <span class="token assign-left variable">Taken</span><span class="token operator">=</span><span class="token number">862</span>.09sec
Fitting <span class="token number">5</span> folds <span class="token keyword">for</span> each of <span class="token number">256</span> candidates, totalling <span class="token number">1280</span> fits
<span class="token assign-left variable">Fold</span><span class="token operator">=</span><span class="token number">2</span>, Accuracy <span class="token operator">=</span> <span class="token number">0.9126</span>, Time <span class="token assign-left variable">Taken</span><span class="token operator">=</span><span class="token number">806</span>.86sec
Fitting <span class="token number">5</span> folds <span class="token keyword">for</span> each of <span class="token number">256</span> candidates, totalling <span class="token number">1280</span> fits
<span class="token assign-left variable">Fold</span><span class="token operator">=</span><span class="token number">3</span>, Accuracy <span class="token operator">=</span> <span class="token number">0.9012</span>, Time <span class="token assign-left variable">Taken</span><span class="token operator">=</span><span class="token number">855</span>.28sec
Fitting <span class="token number">5</span> folds <span class="token keyword">for</span> each of <span class="token number">256</span> candidates, totalling <span class="token number">1280</span> fits
<span class="token assign-left variable">Fold</span><span class="token operator">=</span><span class="token number">4</span>, Accuracy <span class="token operator">=</span> <span class="token number">0.9123</span>, Time <span class="token assign-left variable">Taken</span><span class="token operator">=</span><span class="token number">854</span>.84sec</code></pre>
<h2>Results</h2>
<p>I have decided to submit two versions of the XGBoost models, one without hyperparameter tuning and one after tuning. The accuracy scores I achieved for both submissions are as follows:</p>
<p><strong>XGBoost (Without Tuning)</strong>: 0.90191<br>
<strong>XGBoost (After Tuning)</strong>: 0.90561</p>
<p>The first result kept me between 1973 and 1992 in ranking, while the second result kept me between 1258 and 1279. You can see that the difference of 0.0037 accuracy score shifted the rank by around 715 levels.</p>
<h2>Key Learning</h2>
<p>For certain types of models, you have to label encode and inversely label encode the target variable to get back the results in string format. The reason is that some models don’t work with target variables in string format.<br>
After building the model, you get the result in numerical format. So, you have to inverse the label encoding of the target variable to convert it back into its original string representation.</p>
<ul>
<li>While doing preprocessing, address a situation where test data might have a value not present in the original dataset.</li>
</ul>
<h2>Conclusion</h2>
<p>It was a hectic day, finishing this blog on the second day because I was exhausted yesterday. I wrote the result of only one grid search that took me 71 minutes, but I ran many such grid searches. There is a limit on how much score one can improve by doing hyperparameter optimization. Because it takes so much time, I can’t we can justify the cost of resources. I only have a little time and resources. Because when I ran the model, I couldn’t do anything on my system. So, I decided that from tomorrow onwards; I would check other things, like feature engineering and advanced models, but not hyperparameter optimization, for the 30-day Kaggle challenge. I won’t be able to complete all the problems if I spend so much time on a single problem. My main focus should be covering as many problems as possible. After finishing this challenge, I can improve each problem’s score and write a second blog post about each.</p>
<h2>Links</h2>
<p><strong>Notebook</strong>: <a href="https://www.kaggle.com/code/surajwate/s4e2-prediction-of-obesity-risk">Kaggle Notebook for S4E3</a><br>
<strong>Code</strong>: <a href="https://github.com/surajwate/S4E2-Multi-Class-Prediction-of-Obesity-Risk">GitHub Repository for Day 5</a></p>


    <a href="/blog" class="back-to-blog">← Back to Blog</a>

    <!-- Add Giscus Comment Section Here -->


        <section id="giscus-comments">
            <script src="https://giscus.app/client.js" data-repo="surajwate/surajwate.github.io" data-repo-id="R_kgDOMRXrqQ" data-category="General" data-category-id="DIC_kwDOMRXrqc4CiRIe" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" data-loading="lazy" crossorigin="anonymous" async=""></script>
        </section>
    </div>

    <a href="/blog" class="back-to-blog">← Back to Blog</a>


    </main>
    <footer>
        <div class="footer-container">
            <p>© 2024 Suraj Wate. All rights reserved.</p>
        </div>
    </footer>
    <!-- Prism JS -->
    <script defer="" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/prism.min.js"></script>
    <script defer="" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/components/prism-python.min.js"></script>
    <script>
        // Select DOM elements
        const navToggle = document.querySelector('.nav-toggle');
        const navLinks = document.querySelector('.nav-links');
        // const navItems = document.querySelectorAll('.nav-links li');
        const navItems = document.querySelectorAll('.nav-links a');


        // Toggle menu on hamburger click
        navToggle.addEventListener('click', () => {
        navLinks.classList.toggle('active');
        navToggle.querySelector('.hamburger').classList.toggle('active');
        });

        // Close menu when a nav link is clicked
        navItems.forEach(item => {
        item.addEventListener('click', () => {
            navLinks.classList.remove('active');
            navToggle.querySelector('.hamburger').classList.remove('active');
        });
        });

    </script>
    <script type="module"> 
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.2.0/+esm' 
        mermaid.initialize({ startOnLoad: true });
    </script>
    

</body></html>